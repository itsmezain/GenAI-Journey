Hey guys, my name is Nitish and welcome to my channel! Today, we're going to talk about why language change is so important. We'll also discuss what you can create using language change, and at the end, I'll share some popular alternatives to it. Overall, this video is going to set up some really cool ideas for future videos, so that’s why it’s super important. I'm really excited for this playlist, and I hope you are too. Let's get started!

"Let's kick off this video, so we're starting our discussion about LangChain. The first and most obvious question is, what is LangChain? Let me break it down for you in one simple sentence: LangChain is an open-source framework for developing applications powered by LLMs. So, if you want to create any application based on LLMs, the framework that will help you with that is LangChain. Got it? But honestly, you probably won't grasp the importance of LangChain just from that one sentence."

Feel like whenever you need to learn something, the first thing you should get is why that thing is important, right? If you want to study what X is, you should know why X was needed in the first place. So, I'll do the same thing when explaining what language change is. First, I’ll explain why we need language change. So, let’s move on to the next slide and discuss why we need language change. We need this framework, and trust me, it might take a little while to prove this point, but believe me, when this discussion wraps up, you'll get it.

"I have a really deep perspective and insight on how important language is. So, let me share an example from my life. I've been working in the startup world for quite a while now, and I've started my own startups and explored a ton of ideas over the last 10 years. Back in 2014, I had this idea. At that time, AI and all that stuff weren't really a big deal, at least in India. I noticed that people had started reading a lot of PDFs; before that, they used to read more books, but around that time..."

Back in 2014-2015, smartphones were becoming a bit more private, so people started reading a lot of PDFs. That got me thinking—what if I created an app where anyone could upload their PDFs and read them? Obviously, it's a PDF reader, so you can read them, but it would also have a chat feature. You could click on that chat feature and actually talk about your PDFs. For example, let’s say I have a book on machine learning here.

"I've uploaded it, and this book covers everything about machine learning. So, I can not only read this book, but at the same time, I can ask all sorts of questions through chat. For example, I can ask, 'Hey, explain page five to me like I'm a five-year-old.' And the chatbot can give me a really simplified summary of page five. I might also ask it to generate some true or false questions about linear regression that we've studied, so I can test my understanding."

"Is it okay if we practice, or can I ask this question: can you generate notes on the decision tree we studied in this book? I hope you understand how this kind of application can be incredibly useful, right? Because not only are you reading, but at the same time, you're interacting with your book. Back in 2014, I thought this was a big idea and maybe I should work on it. And guess what? I did! Now, let me tell you how I planned this application."

Let's break down how we're going to build this and what the system design looks like. Alright guys, let’s have a high-level discussion about how we can create this kind of application. Once you understand the high-level overview, we can dive into the details. So, here's how the system works: as soon as a user uploads a PDF to our application, we’ll grab that PDF and store it in our database. Now, when the user opens that PDF, they’ll...

"So, I got a question. What's the deal with the components of linear regression? First off, we need to figure out where in the whole book this topic is discussed—like, on which pages it's mentioned. Basically, we gotta perform a search operation. But there are two types of search operations. One is a normal keyword search. In a normal keyword search, you grab words like 'component' and 'linear regression'."

You're going to search the entire book for those words wherever the term "ampan" or "linear regression" is mentioned, and we'll pull all those pages together. But you can see how that's a bit inefficient, right? What we really need are contextual results about the aspects of linear regression. But in our case, we're getting a ton of extra pages because the word "ampan" might show up in a lot of places, so we're ending up with a bunch of unnecessary pages as results. It's a bit of a hassle.

If it's inefficient, what can we do? We'll do keyword searches all day long, but semantic search is a bit different. I'm sure you've learned about it in NLP. With semantic search, you try to understand the meaning of your query. So, instead of searching through the entire book for "coefficient of linear regression," you directly search for "coefficient of linear regression," and automatically, you'll get results that show you the pages where that term is mentioned.

Since we're focusing on meaningful results, we might end up with fewer pages but they'll be more relevant. So, what we did was take the user's query and perform a semantic search across our entire document. This led us to two pages—page 372 and page 461—where linear regression is discussed. Now that we have these results, we'll take the content from these pages and the user's original query to create a system query.

We'll send it to the most important component of our application, which I'm currently calling the "brain." Alright, this brain has one main purpose—actually, two purposes. The first purpose is to understand the query really well, like to get its meaning clearly. So, it should have Natural Language Understanding capabilities. It should understand English if the question is asked in English, and Hindi if it's asked in Hindi. But the key is that it needs to grasp the text or query thoroughly so it knows what to do next.

The brain should have contact-aware text generation capabilities, which means it needs to know what to do. Essentially, its job is to take a query and a two-page document, understand the query, and pull the answer from that document. To understand the query, it needs natural language understanding capabilities, and then it has to generate a relevant answer based on that context. So basically, our system's brain has two main functions.

The purpose here is to really understand the user's query and then dive into the pages we're providing to generate context-aware text. So, what it will do is read through these pages, extract the five elements of linear regression, and generate text from that to give us as the final output. That's how this whole process works. Now, you might be wondering why we put in all this effort for semantic search if my brain can understand the user's query.

He can find answers by searching within the given pages, so we might as well just give him the whole book. Let's say this book has eight pages. Instead of wasting time sifting through it, wouldn’t it be better to just send him the document for those eight pages and let him know the user asked a question based on that book? Now, think about what the problem is here. It’s a pretty simple issue. Imagine you're in school, and you have a doubt in your math book, so you go to your teacher.

You're telling someone, "Hey, here’s this book, I have a doubt in algebra." This is scenario one. Scenario two is you go to your teacher and say, "Hey, I have a question on page 155." Which scenario do you think will get you a quicker and better response from the teacher? Obviously, it’s the second one because you pointed out a specific page instead of handing over the whole book. The same idea applies here; you’re giving your system's brain two specific pages and a user.

You asked him to check out the query and said, "Look, man, read this query, understand it, and then pull the answer from these two pages." If you give the whole book, it’ll be way more complicated computationally, and the results you get might not even be that great. That’s why we implemented this whole semantic search feature. So, that’s a high-level overview of our entire system. Now, let’s dive a bit deeper and figure out exactly how to build this system in detail.

To really get a grip on this system at a basic level, we first need to understand semantic search. A little while ago, I mentioned that in semantic search, you search based on the meaning of the text. So, let me explain how semantic search actually works. The idea is pretty simple. Imagine you have three paragraphs about three cricketers: Virat Kohli, Jasprit Bumrah, and Rohit Sharma. Now, you’ll be asked a question, and you need to find the answer from those three paragraphs.

You need to figure out which paragraph has the answer hidden in it. So, here's how it works: let's say the question is "How many runs has Virat scored?" We know the answer is in the paragraph about Virat Kohli. But how will our code understand this? First, you convert all your text into embeddings. Converting to embeddings means turning it into a vector, which is basically a set of numbers. So, essentially, you're capturing the semantic meaning of the entire paragraph.

If you want to represent it in the form of numbers, there are many techniques you can use, like word2vec or doc2vec. You can generate embeddings for words—there are tons of techniques, right? The idea is to represent the whole paragraph in the form of multiple numbers. This is a vector for this paragraph and the same for that one. Let's assume for a moment that the dimension of these vectors is 100; basically, it's a 100-dimensional vector.

Alright, we’ve got the vectors for all three paragraphs. So what’s next? As soon as we get a query, we’ll generate an embedding for it too. Basically, we’ll convert it into vector form, creating a 100-dimensional vector. Now, I have four vectors in this high-dimensional space: the three paragraph vectors, which represent Virat Kohli, Jasprit Bumrah, and Rohit Sharma, plus your query vector. So, what are you going to do with your query vector?

"Find the similarities with all three vectors, and the one that shows the strongest similarity will help you figure out which question relates to that paragraph. Then, you can use that paragraph to answer the question. This is how semantic search works, and we need to implement this in our system for our PDFs. So now that we understand how semantic search operates, I'll explain the whole system design in detail. Basically, it'll all kick off as soon as our user uploads a PDF."

Sure, here's the translation:

"If everything goes well, we'll upload the PDF and store it somewhere in the cloud. For now, let's assume we're using AWS services, so we'll store that PDF in AWS's S3 service. Alright, now that we have our PDF in the cloud, what do we need to do next? First, we’ll need to load it. Basically, we need some kind of document loader that will help us bring the PDF into our system. Once it’s in the system, the first thing I need to do is..."

We need to break the PDF into smaller chunks. These chunks can be based on anything – chapters, pages, or even paragraphs. Let's say we have a PDF with 1,000 pages, and we're chunking it based on pages. So, we've divided the whole document into 1,000 parts. Basically, we're using some kind of text splitter that helps us separate each page. Now, what do we do next?

We need to generate embeddings for every page, just like I mentioned a little while ago. So here, we're using some embedding model, and we're sending each page through this model. The embedding model generates an embedding for that page, which basically means it's giving us a vector in an N-dimensional space. Now, I have 1000 vectors, or basically 1000 sets of numbers. So what I’ll do is store these embeddings in a database so that I can query them later on.

We'll discuss a slightly different type of database, but whatever it is, they are databases. We're storing these embeddings in those databases. Now, this is where our main task begins. Our user comes in, opens a PDF, and asks a question. Alright, the question is just text, so what do we do? First, we send this question to the same embedding model to generate an embedding for it, again in the same dimension. Now, what happens is that we already have 1,000 vectors in the database, and now a new vector has come in.

We'll compare all the vectors to basically find the distance, and the vectors or the set of vectors that are closest to our query vector will be the ones we pick. Let's say we've decided to return five vectors; we'll get back the five most similar ones. Then, we'll extract the corresponding pages, and those are the pages we'll find here. I hope that makes sense. After that, the entire flow remains the same as the user's original input.

We'll take the query and these pages, and by combining them, we'll create our system query. This system query will go into the brain of our app, where it'll involve NLU plus context-aware text generation, which will give us our answer. We'll then present that answer to the user as the final output. So, this is a low-level overview. We've discussed all the details, and I really hope that by now, after the time we've spent in this video, you have a clearer understanding of the entire system design.

"Let's kick off our discussion. We've got a good grasp on the system design for this whole project, but let's chat a bit about the challenges we might face while building it. If I had to point out the biggest challenge, it’s definitely the whole flow involved. The most challenging part is creating this brain. Think about it: we need to develop a component that can understand any query we throw at it. That’s already a pretty tough job. Then, once it understands, it has to generate the relevant text, which is also quite a challenge."

It's a challenging job, and honestly, a lot of work has been done in NLP on both of these things. But the real breakthrough happened in 2017 with the Transformer paper. After that, the BERT and GPT papers came out, and that's when the whole LLM saga kicked off. Finally, they cracked this problem. So guess what? We don’t have to put in too much effort to develop this brain; LLMs are already out there with both of these capabilities—natural language understanding and all that.

"Context-aware tax generation is possible now. All we need to do is use a large language model (LLM). It was a big challenge back in 2015, but now it’s not such a big deal. You can easily solve this problem by using an LLM. So, what I'm going to do now is replace 'brain' with 'LLM' in this flow. There are plenty of LLMs available in the market; you can even use an open-source LLM. If you're a big company, you can train your own foundation model to create an LLM."

Hey, so whatever's going on isn't our problem. We figured out how to tackle our first challenge, and the solution is using LLMs. Now, let me tell you about challenge number two. A big challenge is that if you want to use an LLM as your brain, you need to host it somewhere in your system, basically on your server. You might know that LLMs are pretty heavy models, really complex deep learning models trained on data from the entire internet, right?

If you’re trying to run such a large LLM model on your own servers to get results, you’re going to have to do a lot of engineering work. You’ll face a ton of computational problems because it’s not easy to manage and run these big systems on your servers from an engineering perspective. Also, the costs are going to be really high. So, the second big challenge is figuring out how to bring an LLM onto my system or cloud, how to run it, and how to manage the costs. Got it?

The good news is that this challenge has already been solved. Big companies like OpenAI and Anthropic have put their chat models, like ChatGPT, on their own servers and created an API around their LLMs. So what's the benefit of this API? Anyone can interact with these LLMs; you just ask a question, and it goes to the LLM, which replies back to you.

You don't need to download the whole LLM onto your server; you can just hit these APIs and get your work done. So basically, instead of using the LLM directly, I'll be using an LLM API, alright? Plus, the cool thing about using an API is that I only pay for what I use. So if my usage is low, my payment will be low too. In short, the two big challenges of natural language understanding and text generation have been solved by the LLM, and everything around the LLM is computationally efficient.

The challenges they faced have been solved by these APIs, so two major challenges that existed have already been resolved as of 2025. Now, let me tell you about Challenge Number Three in building this system. Challenge Number Three is orchestrating the entire system, meaning being able to bring together all the components here and make them work together is a huge challenge in itself. Alright, let me try to explain this a bit better. First, I want to show you how many moving parts we have here.

First off, you've got your AW A3 component, which is where you're storing your documents. Next up is the text splitter, which is basically a model that decides how to split the documents based on their content. Then there's the embedding part, which we also handle through a model, making it another component. After that, we have the database where you'll store your embeddings, so that's another component. Lastly, your LLM is also a component. So, we’ve got five components in this system. Now, if I move on to the task...

So, basically, we're doing a bunch of different tasks here. We're loading documents, splitting text, handling embeddings, managing the database, retrieving data, and interacting with this LLM. In total, there are about five or six tasks that we need to execute through a pipeline. So, in short, this system has a lot of moving parts.

"We have to execute a lot of tasks between the components, which is pretty challenging. If you decide to write all this code from scratch, it could be really, really difficult. Let me give you an example: let’s say you coded this system manually, and then you find out tomorrow that you can’t use OpenAI’s API anymore because it’s too expensive. Instead, you might be moving to GCP instead of Google3, or maybe you want to use a different model for embeddings. So you get that there are..."

So, there are a lot of moving parts and a ton of interaction between them, and it's pretty complex. Coding all this by hand is a super challenging task. That's where Langchain comes into play. What Langchain does is provide you with built-in functionalities, allowing you to easily plug and play all these components to interact with each other. Not only that, but you can also use Google's tools without relying solely on OpenAI. Basically, it saves you from having to write a ton of boilerplate code for all these components.

You don't really need to worry because all this work is handled behind the scenes by LangChain. So, to sum up this whole conversation, if you want to build an LLM-powered application, the heavy lifting is mostly done by the LLM, but managing that LLM-based app end-to-end with all its moving parts is a really tough task, especially since the technology is still pretty new. That's where LangChain comes into play; it says you just need to focus on your...

"Focus on the idea. I'll handle all the interfacing and orchestration work for you, alright? So now that you've got a bit of an understanding about why LangChain is being used in this whole system, let me tell you about the other benefits that LangChain provides. We’ve tried to get a grasp on the reasons for using LangChain, and now I want to formally discuss its benefits with you. So let's go through them one by one, starting with the very first benefit of LangChain."
-----------------------------------------------------------
	benefit of LangChain
	-------------------------
There's this concept called chains, right? In fact, the name "LangChain" is because of this chain concept. So, with chains, you can take different components and tasks and put them together in a chain-like structure. Basically, you can form a sort of pipeline. You can layer in as many complexities as you need. For instance, in our example, you need to execute multiple tasks between multiple components—like loading a document, splitting text, getting embeddings, and storing everything in a database.

1- So, first, you need to retrieve the data and then send it to the LLM. Now, this is a series of tasks, right? So what you can do is convert this whole pipeline into a chain. And the best part about that chain is that the output of one component automatically becomes the input for the next one, so you don’t have to write all that code manually. Not only that, but you can create really complex chains. You can set up seraial, parallel chains or conditional chains. No matter how complex it gets or how many tasks you have, you can express it in a very clear way with these chains.

You can build that pipeline using LangChain. That's the biggest feature we'll look at next.

 2- After that, there's the second big feature: model agnostic development. It means you can use any model here, it doesn't matter. I gave you an example a little while ago. You can use OpenAI's API or use GOOGLE,  use just have to change only one line of code aur aapka conversion ho jaayega. Here, use AWS or GCP. You quickly here with 1 or 2 line of code, entire codebase will be shifted to a different component.

Basically, model agnostic means you should focus on your core logic and business logic. You can move components around wherever you want, it doesn't matter. 

3- The third is LangChain has made an effort to provide you with a complete ecosystem. If you need a document loader, there are all kinds available. You can even pull data from the cloud, load excel files, or load PDFs—pretty much anything can be loaded. In the text splitter, you'll find 50 different types of splits available. There are tons of embedding models in the embed section. You have a wide range of databases at your disposal. So basically, whatever component you use, there are plenty of varieties available for your company's product.

Whatever component you want to work with in LangChain, the interface is available. So, it’s never going to be like, "Man, we can't implement this component in LangChain." 

4- Also, here you get the concept of memory and state handling. For example, let’s say our user asked, "What are the assumpation of linear regression?" and our system provided those assumpation Then, they immediately asked the next question, "Also, give me a few interview questions on this machine learning algorithm." But on which machine learning algorithm are we talking about,the previous query we don;t remember?
 
We don't  remember that last time we talked about the linear regression so this problem is also solved by LangChain. There's this memory concept where you can use conversational memory. So, if we're discussing linear regression in one conversation, our model can still understand that it's about linear regression even if we don't mention it later. And all of this is provided by LangChain. So, 


in short, it's a really great tool and an awesome library for you."LLM-powered applications are super helpful, right? 

--------------------------------------------------------------
what you can create using LangChain or what people around the world are building with it. There are multiple use cases."

1-You can create all sorts of things. Let me give you a few examples. The first and most popular use case is that you can use LangChain to build conversational chatbots. We're living in the internet era, and most of the companies around us are internet-based, like Swiggy and others. The biggest problem these internet-based companies face is scaling, which means dealing with a lot of customers at once, right? So, when it comes to talking to customers, one way to do it is to set up your call center. Setting up a call center and hiring a ton of people is a huge challenge, right? What if I had a chatbot that could talk just like a call center executive, understand the user's queries, and provide solutions? That would solve a big problem for me as a business. So, a lot of companies these days are creating chatbots. Now, what happens is that the first layer of communication between the company and the customer is handled by these chatbots. And when the chatbots can't handle it  we forward it to a human, and then the human handles it from there, alright? So, the most popular use case is that you can use Langchain to create conversational chatbots, and a lot of people are doing that. 


2-The second use case is AI knowledge assistants. You can think of an AI knowledge assistant as a chatbot, but it also has access to your data, kind of like it’s trained on your data. For example, let’s say we have a website for our campus where our courses are listed, and now I want to integrate a chat bot into our courses. So, whenever a student is watching a lecture video and has a doubt, they can quickly ask the chat bot about that specific question. The cool thing is that this chat bot will also have knowledge about the lecture content and what’s being discussed. You can easily build this knowledge assistant with the help of Langchain, and we’ll be moving forward with this as well."

3-The use case for AI agents is a really popular term that you've probably heard a lot over the past year. So, agents are basically chatbots on steroids. They don’t just chat; they can actually do stuff too. Like take MakeMyTrip for example—people use it to book hotels, train tickets, and flights. Generally, older folks, like those in their 60s, aren’t that fluent when it comes to using these kinds of websites.

"So, what can we do about booking tickets and stuff? We can set up an agent here, like an AI agent, that will not only talk like a human but can also do things for you when you ask. For example, if a senior citizen is chatting with this AI agent, they can just say something like, 'I need to book the cheapest flight tickets between this place and that place on this date,' and this AI agent has the tools and power to execute the whole task on its own. That's what people are saying about AI agents."

AI is the next big thing, and it's likely to become even bigger. The cool part is that you can use LangChain to create agents. In fact, we'll show you how to build a basic agent in this playlist. It's definitely an interesting use case, but there are plenty of others too.


4- You can automate workflows at any level—personal, professional, or company-wide—using AI. So, LangChain is a great tool for automating workflows. 


5-Summarazation / Research HelpersWhat you can do is use LangChain to simplify research papers or books and stuff. You probably already know that you can’t upload huge books to ChatGPT because of the content length issue. Also, another problem might be that sometimes you can’t upload your company’s private data to ChatGPT because your company doesn’t allow it. So, what you can do is have your company use LangChain to create a tool similar to ChatGPT.
---------------------------------------------------------------

It can process even the biggest documents and answer related questions. Plus, since it’s your company’s personal chatbot, you can upload private data and chat about it. Tools like this for summarization and research are popping up a lot using language models. And who knows what other tools will come out in the future? The future looks bright; it seems like we’ll see even more LLM-based applications, kind of like the boom we had with websites.

It feels like we're on the brink of a boom in LLM-based applications, similar to the surge we saw with apps before. And LangChain is going to play a huge role in that. One last thing I want to mention in this intro video is that LangChain isn’t the only framework out there to help you build LLM applications. There are actually several other frameworks too. Two of the more popular ones you might hear about are LlamaIndex and Haystack. Both of these frameworks are also quite popular and widely used.

All the companies are using these frameworks instead of relying solely on LangChain. It really depends on where you're getting better pricing and which tool feels right to you; that's how decisions are made. But yeah, LLM is a bit more popular, you might have heard of it. We even have a course on it on our website. The second one is pretty similar, kind of like a library and framework where you can easily build LLM-based applications. If you want, I can show you one.

I can provide a comparative study between the Lang Chain and Hec, but I feel like since we haven't fully explored Lang Chain yet, it's not the right time. In the future, I will definitely dive into this topic and give you an analysis of the pros and cons between Lang Chain and Hec. For now, you just need to know that there are other frameworks out there as well. So, with that, I've introduced you to Lang Chain, and now you have an idea about it.

"What is a language model and why do we need it? What can you do with it? Now we’re ready to move forward in our journey. In the next video, we'll discuss the complete ecosystem of language models, and trust me, it’s really important to understand that. Once we get that down, we’ll jump into the practical parts, start coding, and begin creating LLM-based applications. So if you liked this video, please like, share, and subscribe. See you in the next video, bye!"

